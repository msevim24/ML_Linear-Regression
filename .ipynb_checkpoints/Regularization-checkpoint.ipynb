{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19123c-b019-4e9b-9936-d8372a5885a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c1dae-1520-4d21-af52-bec72622cc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57153564-c4e1-4487-9bf1-45fe76e4f7a3",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1002e-64ed-4fda-ad95-b7ef9a1a362b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c787ef-d8e1-44e9-ad78-9882ff6d7a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9e9f1-4ff6-44ba-86f1-e0e47f5dbd65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "057b0940-07d5-45ca-b1c4-b05039296027",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "* **Some Machine Learning models** that rely on distance metrics (e.g. KNN) **require scaling** to perform well.\n",
    "* Feature scaling improves the convergence of steepest descent algorithms, which do not possess the property of scale invariance.\n",
    "* If features are on different scales, certain weights may update faster than others since the feature values play a role in weight updates.\n",
    "* **Critical benefit of feature scaling related to gradient descent.**\n",
    "* Tehre are some ML algorithms wherre scaling will not have an effect (e.g. CART based methods).\n",
    "* Scaling the features so that their respective ranges are **uniform is important in comparing measurements** that have different units.\n",
    "* It allows us directly compare model coefficients to each other.\n",
    "* **Must always scale new unseen data before feeding to model.**\n",
    "* Effects direct interpretability of feature coefficients.\n",
    "* **Feature scaling benefits:**\n",
    "  \n",
    "> Can lead to great increases in performance.\n",
    "\n",
    "> Absolutely necessary for some models.\n",
    "\n",
    "> Virtually no \"real\" downside to scaling features.\n",
    "\n",
    "#### Two main ways to scale features:\n",
    "  \n",
    "**1. Standardization:** Rescales data to have **a mean of 0 and standard deviation of 1**. It is also called **Z-score normalization**.\n",
    "\n",
    "**2. Normalization:** Rescales all data values to be **between 0-1**.\n",
    "\n",
    "* There are many more methods of scaling features and **Scikit-Learn** provides easy to use classes that **fit** and **transform** feature data for scaling.\n",
    "* A **.fit()** call simply calculates the necessary statistics (Xmin, Xmax, mean, standard deviation). \n",
    "* A **.transform()** call actually scales data and returns the new scaled version of data.\n",
    "* We **only fit to training data**. Calculating statistical information should only come from training data.,\n",
    "* Using the full data set would cause **data leakage**.\n",
    "#### Feature scaling process:\n",
    "  \n",
    "> Perform train-test split\n",
    "\n",
    "> Fit to training feature data\n",
    "\n",
    "> Transform training feature data\n",
    "\n",
    "> Transfrm test feature data\n",
    "\n",
    "#### Do we need to scale the label (y)?\n",
    "* In general, it is not necessary nor advised.\n",
    "* Normalizing the output distribution is altering the definition of the target.\n",
    "* Can negatively impact stochastic gradient descent.\n",
    "* So, we only scale the features (X).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1790d-7300-479d-b998-c9735cbe068d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d8496-91b5-48be-a425-9448978583ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9dc4437-7239-481f-9d6f-30aaf52a4404",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140e23a-d2a3-4cff-9aad-3d41ec52b34b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "355241e1-8010-49ef-adf9-e64d10946742",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Regularization seeks **to solve a few common model issues** by:\n",
    "- Minimizing model complexity\n",
    "- Penalizing the loss function\n",
    "- Reducing model overfitting (add more bias to reduce model variance)\n",
    "  \n",
    "In general, we can think of regularization as a way to reduce model overfitting and variance.\n",
    "\n",
    "**Three main types of Regularization:**\n",
    "\n",
    "**1.** L1 Regularization: **LASSO Regression**\n",
    "\n",
    "**2.** L2 Regularization: **Ridge Regression**\n",
    "\n",
    "**3.** Combining L1 and L2: **Elastic Net**\n",
    "\n",
    "These regularization methods have a **cost**:\n",
    "* Introduce a additional hyperparameter that needs to be tuned.\n",
    "* A multiplier to the penalty to decide the \"strength\" of the penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d97de-3ead-4742-8114-aa414ecb733b",
   "metadata": {},
   "source": [
    "## 1. LASSO Regression (L1 Regularization)\n",
    "* L1 regularization adds a penalty equal to the **absolute value** of the magnitude of coefficients.\n",
    "* It limits the size of the coefficients.\n",
    "* It can yield sparse models where **some coefficients can become zero**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a7b1b-de71-4a0e-8c31-38c96f7603c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7f251-37b9-4a6f-85ff-45d52f6be9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b12708-0e3f-4581-ab16-c1444fdd930c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c4a5f-3bbd-43ab-845a-31b8dfc0f36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7b508-67c0-40f9-af58-7ec8f4a90e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ff424a7-b54e-4c43-8303-be84d5dd9747",
   "metadata": {},
   "source": [
    "## 2. Ridge Regression (L2 Regularization)\n",
    "* L2 regularization adds a penallty equal to the **square** or the magnitude of coefficients.\n",
    "* **All coefficients are shrunk** by the same factor.\n",
    "* Does not necessarily eliminate coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d9c8d-33ea-4621-8d1f-2a2d3cf0d0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e29df-2cae-49d7-9ce1-466d85a66356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da6ba2d4-cd97-4f52-8c3a-d307daaca34d",
   "metadata": {},
   "source": [
    "## 3. Elastic Net\n",
    "* Elastic Net **combines L1 and L2 with the addition of an alpha parameter** deciding the ratio between them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
